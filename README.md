# âš–ï¸ Chatbot TÆ° Váº¥n PhÃ¡p LÃ½ Viá»‡t Nam (RAG)
Trá»£ lÃ½ AI cÃ³ vai trÃ² nhÆ° má»™t luáº­t sÆ° giÃºp tÆ° váº¥n cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n phÃ¡p luáº­t Viá»‡t Nam dá»±a trÃªn tri thá»©c Ä‘Æ°á»£c truy xuáº¥t (RAG). Há»‡ thá»‘ng káº¿t há»£p FAISS (dense) + BM25 (sparse) + Cross-Encoder Reranker Ä‘á»ƒ chá»n lá»c ngá»¯ cáº£nh, sinh Ä‘Ã¡p Ã¡n báº±ng mÃ´ hÃ¬nh trÃªn Ollama, vÃ  cÃ³ bá»™ nhá»› Ä‘á»‡m (cache) tiáº¿t kiá»‡m thá»i gian truy váº¥n.

## ğŸ“Š Káº¿t quáº£
![Sample Image](demo/demo.png)


## ğŸ¯ TÃ­nh nÄƒng
- **RAG Hybrid**: FAISS + BM25 (káº¿t há»£p báº±ng EnsembleRetriever).
- **Reranking**: `BAAI/bge-reranker-base` tÄƒng cháº¥t lÆ°á»£ng káº¿t quáº£ truy xuáº¥t.
- **LLM (Ollama)**: gá»i mÃ´ hÃ¬nh tá»« Ollama server (vÃ­ dá»¥ `incept5/llama3.1-claude:latest`).
- **Dá»¯ liá»‡u**: tá»± Ä‘á»™ng táº£i bá»™ `namphan1999/data-luat` (Hugging Face datasets).
- **UI**: Streamlit, chat realtime, xÃ³a cache, lÃ m má»›i cuá»™c há»™i thoáº¡i.
- **Cache**:
  - Cache tÃ i nguyÃªn náº·ng: CrossEncoder vÃ  dataset dÃ¹ng `st.cache_resource` (trÃ¡nh táº£i láº¡i má»—i rerun).
  - Cache cÃ¢u tráº£ lá»i: lÆ°u JSON vÃ  tÃ¬m tÆ°Æ¡ng Ä‘á»“ng ngá»¯ nghÄ©a Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng.

## ğŸ§± Cáº¥u trÃºc thÆ° má»¥c

```
â”œâ”€â”€ main.py                                 # á»¨ng dá»¥ng Streamlit (UI + luá»“ng chat)
â”œâ”€â”€ requirements.txt                        # ThÆ° viá»‡n cáº§n cÃ i Ä‘áº·t
â”œâ”€â”€ chunks_data/
â”‚ â””â”€â”€ load_data.py                          # Táº£i bá»™ 'namphan1999/data-luat'
â”œâ”€â”€ retriever/
â”‚ â””â”€â”€ retriever.py                          # FAISS + BM25 â†’ EnsembleRetriever
â”œâ”€â”€ rerank/
â”‚ â””â”€â”€ reranker.py                           # CrossEncoderReranker (bge-reranker-base) + cache model
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ embedder.py                           # Embeddings cho Vector DB (Ollama) + HF cho cache
â”‚ â””â”€â”€ llm.py                                # Káº¿t ná»‘i Ollama + hÃ m sinh tráº£ lá»i (stream)
â”œâ”€â”€ cache_communicate/
â”‚ â””â”€â”€ caching_communicate.py                # Cache káº¿t quáº£ Q&A báº±ng tÆ°Æ¡ng Ä‘á»“ng cosine
â””â”€â”€ create_suggestions/                     # (tÃ¹y chá»n) sinh gá»£i Ã½ tiáº¿p theo (hiá»‡n Ä‘ang Ä‘á»ƒ máº«u)
```

## ğŸ› ï¸ CÃ i Ä‘áº·t

YÃªu cáº§u:
- ÄÃ£ cÃ i vÃ  cháº¡y Ollama: `ollama serve`
- ÄÃ£ pull model phÃ¹ há»£p, vÃ­ dá»¥:
  - `ollama pull incept5/llama3.1-claude:latest`
  - `ollama pull embeddinggemma:300m` náº¿u dÃ¹ng cho vector DB

CÃ i thÆ° viá»‡n:
```bash
pip install -r requirements.txt
```

Náº¿u cáº§n CUDA cho PyTorch, vui lÃ²ng cÃ i theo hÆ°á»›ng dáº«n chÃ­nh thá»©c cá»§a PyTorch tÆ°Æ¡ng á»©ng GPU/CUDA cá»§a báº¡n.

## ğŸš€ Cháº¡y á»©ng dá»¥ng

```bash
streamlit run main.py
```

Má»Ÿ giao diá»‡n:
- DÃ¹ng nÃºt "XÃ³a bá»™ nhá»› Ä‘á»‡m" Ä‘á»ƒ dá»n cache tráº£ lá»i.
- DÃ¹ng nÃºt "LÃ m má»›i há»™i thoáº¡i" Ä‘á»ƒ báº¯t Ä‘áº§u má»™t cuá»™c trÃ² chuyá»‡n má»›i.

## â“ Lá»—i thÆ°á»ng gáº·p & kháº¯c phá»¥c

- Ollama khÃ´ng pháº£n há»“i:
  - Kiá»ƒm tra `ollama serve` Ä‘ang cháº¡y.
  - Kiá»ƒm tra Ä‘Ã£ `ollama pull` Ä‘Ãºng tÃªn model.
  - Máº·c Ä‘á»‹nh `base_url="http://localhost:11434"`.

- Táº£i model reranker cháº­m má»—i láº§n rerun:
  - ÄÃ£ Ä‘Æ°á»£c cache báº±ng `@st.cache_resource`. Náº¿u váº«n cháº­m, kiá»ƒm tra Ä‘Æ°á»ng truyá»n khi láº§n Ä‘áº§u táº£i model tá»« Hugging Face.

## ğŸ“„ Báº£n quyá»n & LiÃªn há»‡
- Má»¥c Ä‘Ã­ch há»c táº­p/nghiÃªn cá»©u.
- Dataset: `namphan1999/data-luat` (Hugging Face).
- LiÃªn há»‡: `nguyenphuongv07@gmail.com`.
