from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from retriever.retriever import show_retrieved_chunks
from rerank.reranker import show_rerank_chunks

def get_llm(model_choice):
    """
    Tráº£ vá» LLM object cá»§a LangChain
    """
    return ChatOllama(
        model=model_choice,
        temperature=0.001,
        base_url="http://localhost:11434",
        device="cuda"  # Cháº¡y trÃªn gpu
    )

def get_answer_from_llm(chat_history, user_question, retriever_obj, rerank_obj, model):
    # Láº¥y cÃ¡c tÃ i liá»‡u liÃªn quan
    retrieved_docs = show_retrieved_chunks(retriever_obj, user_question)
    reranked_docs = show_rerank_chunks(rerank_obj, user_question)
    context = "\n".join(doc.page_content for doc in reranked_docs)
    print("ğŸ“ Context length:", len(context), "kÃ½ tá»±")
    print(f"Context for LLM:\n{context}\n{'-'*50}\n")

    # Táº¡o ChatPromptTemplate
    prompt = ChatPromptTemplate.from_messages([
        ("system", """"Báº¡n lÃ  má»™t chuyÃªn gia am hiá»ƒu luáº­t phÃ¡p Viá»‡t Nam. Tráº£ lá»i báº±ng Tiáº¿ng Viá»‡t.
         Chá»‰ tráº£ lá»i dá»±a trÃªn thÃ´ng tin cÃ³ trong tÃ i liá»‡u vÃ  lá»‹ch sá»­ há»™i thoáº¡i Ä‘Ã£ Ä‘Æ°á»£c cung cáº¥p bÃªn dÆ°á»›i.
         Tuyá»‡t Ä‘á»‘i KHÃ”NG Ä‘Æ°á»£c suy Ä‘oÃ¡n, bá»‹a Ä‘áº·t hoáº·c Ä‘Æ°a thÃ´ng tin ngoÃ i pháº¡m vi tÃ i liá»‡u.
         HÃ£y tráº£ lá»i rÃµ rÃ ng, chÃ­nh xÃ¡c, trÃ­ch dáº«n Ä‘iá»u luáº­t náº¿u cÃ³ thá»ƒ.

        --- Lá»‹ch sá»­ há»™i thoáº¡i ---
        {chat_history}

        --- TÃ i liá»‡u há»— trá»£ ---
        {context}
        """
        ),
        ("human", "{question}")
    ])

    # Format prompt vá»›i dá»¯ liá»‡u thá»±c táº¿
    messages = prompt.format_messages(
        chat_history=chat_history,
        context=context,
        question=user_question
    )

    # Streaming tá»«ng chunk ná»™i dung
    for chunk in model.stream(messages):
        if hasattr(chunk, "content") and chunk.content:
            yield chunk.content