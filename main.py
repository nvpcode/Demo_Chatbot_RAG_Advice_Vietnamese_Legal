import streamlit as st
from datetime import datetime
from langchain_community.vectorstores import FAISS
from langchain_community.chat_message_histories import StreamlitChatMessageHistory

from models.embedder import get_embedding_model
from retriever.retriever import get_hybrid_retriever
from rerank.reranker import get_rerank_from_retriever
from models.llm import get_answer_from_llm, get_llm
from chunks_data.load_data import get_allchunks_from_knowledges

from cache_communicate.caching_communicate import (
    retrieve_from_cache, 
    store_response, 
    purge_cache
)


# ----------------------------
# Setup Page
# ----------------------------
def setup_page():
    st.set_page_config(
        page_title="ChatBot Ph√°p Lu·∫≠t Vi·ªát Nam",
        page_icon="‚öñÔ∏è",
        layout="wide"
    )


# ----------------------------
# Sidebar Config
# ----------------------------
def setup_sidebar():
    st.sidebar.header("Ch·ª©c NƒÉng")
    retriver_top_k = 10  # st.sidebar.slider("S·ªë t√†i li·ªáu t·ªëi ƒëa mu·ªën truy v·∫•n (top_k_retriver)", min_value=3, max_value=10, value=10, step=1)
    rerank_top_k = 3     # st.sidebar.slider("S·ªë t√†i li·ªáu sau rerank", min_value=1, max_value=3, value=3, step=1)

    if st.sidebar.button("üßπ X√≥a b·ªô nh·ªõ ƒë·ªám"):
        purge_cache()
        st.sidebar.success("ƒê√£ x√≥a cache.")


    st.sidebar.markdown("---")
    st.sidebar.info("D·ªØ li·ªáu ƒë∆∞·ª£c t·∫£i s·∫µn t·ª´ b·ªô `namphan1999/data-luat`.")

    return retriver_top_k, rerank_top_k


# ----------------------------
# Initialize app (load, embed, retriever, rerank, llm)
# ----------------------------
@st.cache_resource(show_spinner=True)
def bootstrap_system(retriver_top_k: int, rerank_top_k: int):
    # 1) Load knowledge (m·ªói ph·∫ßn t·ª≠ l√† m·ªôt chunk t√†i li·ªáu)
    allchunks = get_allchunks_from_knowledges()

    # 2) Embedding + Vector store
    embeddings = get_embedding_model()
    vector_store = FAISS.from_documents(allchunks, embeddings)

    # 3) Hybrid retriever (FAISS + BM25)
    retriever_obj = get_hybrid_retriever(vector_store, allchunks, top_k=retriver_top_k, weights=(0.6, 0.8))

    # 4) Reranker
    rerank_obj = get_rerank_from_retriever(retriever_obj, top_k=rerank_top_k)

    return allchunks, retriever_obj, rerank_obj


# ----------------------------
# Chat UI
# ----------------------------
def setup_chat_interface(model_choice):
    col1, col2 = st.columns([6, 1])
    with col1:
        st.title("‚öñÔ∏è Tr·ª£ l√Ω Ph√°p Lu·∫≠t Vi·ªát Nam")
    with col2:
        if st.button("üîÑ L√†m m·ªõi h·ªôi tho·∫°i"):
            st.session_state.messages = [
                {"role": "assistant", "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam."}
            ]
            st.session_state.input_enabled = True
            st.rerun()

    st.caption(f"üöÄ Tr·ª£ l√Ω AI ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi {model_choice}")

    msgs = StreamlitChatMessageHistory(key="langchain_messages")

    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam."}
        ]
        msgs.add_ai_message("Xin ch√†o! T√¥i c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam.")

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ h·ªôi tho·∫°i
    for msg in st.session_state.messages:
        role = "assistant" if msg["role"] == "assistant" else "human"
        st.chat_message(role).write(msg["content"])

    return msgs


# ----------------------------
# Handle user input
# ----------------------------
def handle_user_input(msgs, retriever_obj, rerank_obj, model):
    
    if input_user := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam..."):

        # L∆∞u v√† hi·ªÉn th·ªã tin nh·∫Øn ng∆∞·ªùi d√πng
        st.session_state.messages.append({"role": "human", "content": input_user})
        st.chat_message("human").write(input_user)
        msgs.add_user_message(input_user)

        # L·∫•y l·ªãch s·ª≠ chat g·∫ßn nh·∫•t ƒë·ªÉ l√†m ng·ªØ c·∫£nh h·ªôi tho·∫°i
        max_history_pairs = 3
        history_messages = st.session_state.messages[-(max_history_pairs * 2 + 1):-1]
        chat_history_str = "\n".join(
            f"{'Ng∆∞·ªùi d√πng' if m['role'] == 'human' else 'Bot'}: {m['content']}"
            for m in history_messages
        )

        with st.chat_message("assistant"):
            placeholder = st.empty()
            
            # 1) Ki·ªÉm tra cache
            cached_result = retrieve_from_cache(input_user)
            if cached_result:
                # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi t·ª´ cache
                cached_answer = cached_result.get("response", "")
                placeholder.markdown(cached_answer)
                streamed_text = cached_answer
                st.info("üíæ ƒêang s·ª≠ d·ª•ng c√¢u tr·∫£ l·ªùi t·ª´ cache")
            else:
                # 2) G·ªçi LLM (c√≥ streaming)
                try:
                    streamed_text = ""
                    for token in get_answer_from_llm(chat_history_str, input_user, retriever_obj, rerank_obj, model):
                        streamed_text += token
                        placeholder.markdown(streamed_text)
                    answer = streamed_text  # ƒë·∫£m b·∫£o c√≥ bi·∫øn answer ƒë·ªÉ l∆∞u cache
                except Exception as e:
                    answer = f"Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi: {e}"
                    placeholder.markdown(answer)

                # 3) L∆∞u cache
                extra_info = {
                    "model": str(model),
                    "chat_history_length": len(history_messages),
                }
                store_response(input_user, answer, extra_info)
                st.info("üÜï C√¢u tr·∫£ l·ªùi m·ªõi ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o cache")

            # L∆∞u v√†o session
            st.session_state.messages.append({"role": "assistant", "content": streamed_text})
            msgs.add_ai_message(streamed_text)

# ----------------------------
# Main
# ----------------------------
def main():
    setup_page()
    retriver_top_k, rerank_top_k = setup_sidebar()

    # Kh·ªüi t·∫°o h·ªá th·ªëng (d·ªØ li·ªáu, retriever, rerank)
    _, retriever_obj, rerank_obj = bootstrap_system(retriver_top_k, rerank_top_k)

    # Kh·ªüi t·∫°o model LLM
    model_LLM = "incept5/llama3.1-claude:latest"
    model = get_llm(model_LLM)

    # Giao di·ªán chat
    msgs = setup_chat_interface(model_LLM)
    handle_user_input(msgs, retriever_obj, rerank_obj, model)


if __name__ == "__main__":
    main()
